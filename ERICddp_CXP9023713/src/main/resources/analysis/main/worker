#!/bin/bash

if [ -x /usr/sfw/bin/gtar ] ; then
    GTAR=/usr/sfw/bin/gtar
elif [ -x /bin/gtar ] ; then
    GTAR=/bin/gtar
else
    echo "ERROR: gtar not found, aborting"
    exit 1
fi

FQDN=$(cat /data/ddp/env.php | grep "ddp_site_domainname" | sed "s/^.*\s*=\s*['\"]\(.*\)['\"].*$/\1/g")
if [ -z "${FQDN}" ] ; then
    FQDN=$(hostname -f)
fi

INSTANCE=$(echo "${FQDN}" | cut -d'.' -f1)

log() {
    MSG=$1
    LOG_TIMESTAMP=$(date "+%y-%m-%d %H:%M:%S")
    echo "${LOG_TIMESTAMP} Worker ${MSG}"
}

archiveFile()
{
    local DATA_FILE=$1
    local SITE_NAME=$2

    if [ -d ${ARCHIVE_DIR} ] ; then
        if [ ! -d ${ARCHIVE_DIR}/${SITE_NAME} ]; then
            mkdir ${ARCHIVE_DIR}/${SITE_NAME}
        fi

        # we should have a vlid directory now
        FILE=`basename ${DATA_FILE}`
        /bin/mv -f ${DATA_FILE} ${ARCHIVE_DIR}/${SITE_NAME}/${FILE}
        if [ $? -ne 0 ] ; then
            log "ERROR: Could not archive ${DATA_FILE} to ${ARCHIVE_DIR}/${SITE_NAME}/${FILE}"
        else
            # Make sure the file is world read-able
            chmod 644 ${ARCHIVE_DIR}/${SITE_NAME}/${FILE}
        fi
    else
        log "Archive dir ${ARCHIVE_DIR} doesn't exist"
    fi
}

unpackArchive() {
    local DATA_FILE=$1
    local SITE_TYPE=$2
    local SITE_NAME=$3
    local DATE=$4
    local FILE_SIZE=$5
    local SITE_ID=$6
    local SITE_DIR=$7
    local FILE_TYPE=$8

    #
    # First check that the file isn't still uploading (re-uploading)
    #
    local SECS_SINCE_MODIFIED=$(${ANALYSIS_BIN}/main/lastModified ${DATA_FILE})
    if [ ${SECS_SINCE_MODIFIED} -lt ${MIN_SEC_SINCE_MODIFIED} ] ; then
        DELETE_DATA_FILE=0
        ERROR="File modified ${SECS_SINCE_MODIFIED} ago"
        return
    fi


    #
    # If the file does not match the exact expected file name throw an error.
    # This will prevent 2 different files from being processed for the same date at the same time.
    #
    if [ "${FILE_TYPE}" = "Data" ] ; then
        if [[ ! $DATA_FILE =~ ^.*\/DDC_Data_[0-9]{6}\.tar\.gz$ ]] ; then
            [[ $DATA_FILE =~ ^.*\/(.*)$ ]]
            FILE_NAME=${BASH_REMATCH[1]}
            EXPECTED_FILE_NAME="DDC_Data_${DATE}.tar.gz"
            ERROR="File name <b>${FILE_NAME}</b> does not match the expected file name <b>${EXPECTED_FILE_NAME}</b>, this file will not be processed."
            return
        fi
    fi

    #
    # If we already processed this file before, then log an error
    #
    local ARCHIVE_FILE=$(basename ${DATA_FILE})
    ARCHIVE_FILE="${ARCHIVE_DIR}/${SITE_NAME}/${ARCHIVE_FILE}"
    if [ -r ${ARCHIVE_FILE} ] ; then
        local ARCHIVE_FILE_INFO=$(ls -l --time-style "+%Y-%m-%d %H:%M:%S" ${ARCHIVE_FILE})
        local ARCHIVE_FILE_SIZE=$(echo ${ARCHIVE_FILE_INFO} | awk '{print $5}')
        local ARCHIVE_FILE_TIME=$(echo ${ARCHIVE_FILE_INFO} | awk '{print $6, $7}')
        if [ ${ARCHIVE_FILE_SIZE} -eq ${FILE_SIZE} ] ; then
            ERROR="Already processed at ${ARCHIVE_FILE_TIME}, this file will not be processed"
            return
        fi
    fi

    #
    # For Delta archives check that the file index is the current ARCHIVE INDEX + 1
    #
    if [ "${FILE_TYPE}" = "Delta" ] ; then
        local FILE_ONLY=$(basename ${DATA_FILE})
        local FILE_INDEX=$(echo ${FILE_ONLY} | awk -F_ '{print $4}' | sed 's/\.tar\.gz$//')
        if [ -s ${SITE_DIR}/data/${DATE}/ARCHIVE_INDEX ] ; then
            local CURRENT_INDEX=$(cat ${SITE_DIR}/data/${DATE}/ARCHIVE_INDEX)
            local EXPECTED_INDEX=$(expr ${CURRENT_INDEX} + 1)
            if [ ${FILE_INDEX} -ne ${EXPECTED_INDEX} ] ; then
                if [ ${FILE_INDEX} -gt ${CURRENT_INDEX} ] ; then
                    ERROR="DELTA file ${FILE_INDEX} out of sequence, expecting ${EXPECTED_INDEX}, re-queuing"
                    # Touch the file to slow down it being re-queued
                    touch ${DATA_FILE}
                    DELETE_DATA_FILE=0
                    return
                else
                    ERROR="DELTA file ${FILE_INDEX} out of sequence, expecting ${EXPECTED_INDEX}, not processing"
                    return
                fi
            fi
        else
            ERROR="DELTA file ${FILE_INDEX} out of sequence, valid ARCHIVE_INDEX not found, re-queuing"
            touch ${DATA_FILE}
            DELETE_DATA_FILE=0
            return
        fi
    fi

    [ ! -d ${SITE_DIR}/data/staging_area ] && mkdir -p ${SITE_DIR}/data/staging_area
    cd ${SITE_DIR}/data/staging_area
    if [ $? -ne 0 ] ; then
        ERROR="Failed to cd to ${SITE_DIR}/data/staging_area"
        return
    fi

    #
    # Decompress/Untar
    #
    ${GTAR} xf ${DATA_FILE}
    if [ $? -ne 0 ]; then
        local SIZE=$(ls -l ${DATA_FILE} | awk '{print $5}')
        local SUM=$(sum ${DATA_FILE})
        ERROR="Failed to decompress/untar ${DATA_FILE} size=${SIZE} sum=$SUM ${LS_LINE}"
        return
    fi

    #
    # Move DDC_Data tar file to NAS archive
    #
    if [ "${FILE_TYPE}" = "Data" ] || [ -r ${SITE_DIR}/KEEP_DELTA ] ; then
        archiveFile ${DATA_FILE} ${SITE_NAME}
    fi

    #
    # Check account via ddp.txt
    #
    local FTP_ACCOUNT_DIR=$(dirname ${DATA_FILE})
    local FTP_ACCOUNT_NAME=$(basename ${FTP_ACCOUNT_DIR})
    if [ -r ${DATE}/ddp.txt ] ; then
        local FILE=`basename ${DATA_FILE}`
        local DATA_FILE_DDP_ACCOUNT=`cat ${DATE}/ddp.txt`
        if [ "${DATA_FILE_DDP_ACCOUNT}" != "${FTP_ACCOUNT_NAME}" ] ; then
            /data/ddp/current/server_setup/ddptxtNotFoundMail --instance ${INSTANCE} --ddpTxtContent ${DATA_FILE_DDP_ACCOUNT} --ftpuser ${FTP_ACCOUNT_NAME} --file ${FILE}
            ERROR="ERROR: ddp.txt content, ${DATA_FILE_DDP_ACCOUNT}, does not match FTP account name ${FTP_ACCOUNT_NAME}, file will not be processed"
            rm -rf ${DATE}
            return
        fi
    fi

}

extractAndProcess() {
    local DATA_FILE=$1
    local SITE_TYPE=$2
    local SITE_NAME=$3
    local DATE=$4
    local FILE_SIZE=$5
    local SITE_ID=$6

    local SITE_DIR=${DATA_DIR}/${SITE_TYPE}/${SITE_NAME}
    if [ ! -d ${SITE_DIR} ] ; then
        ERROR="Cannot find site dir ${SITE_DIR}"
        return
    fi

    # Verify file is a supported type Data/Delta
    local FILE_ONLY=$(basename ${DATA_FILE})
    local FILE_TYPE=$(echo "${FILE_ONLY}" | awk -F_ '{print $2}')
    if [ "${FILE_TYPE}" != "Data" ] && [ "${FILE_TYPE}" != "Delta" ] && [ "${FILE_TYPE}" != "LoadMetrics" ] ; then
        ERROR="Unknown file type ${FILE_TYPE}"
        return
    fi

    if [ "${FILE_TYPE}" = "Data" ] || [ "${FILE_TYPE}" = "Delta" ] ; then
        unpackArchive ${DATA_FILE} ${SITE_TYPE} ${SITE_NAME} ${DATE} ${FILE_SIZE} ${SITE_ID} ${SITE_DIR} ${FILE_TYPE}
        if [ ! -z "${ERROR}" ] ; then
            return
        fi
    elif [ "${FILE_TYPE}" = "LoadMetrics" ] ; then
        local DATA_DIR=${SITE_DIR}/data/${DATE}
        if [ -d ${DATA_DIR} ] ; then
            cp ${DATA_FILE} ${DATA_DIR}/loadmetrics.txt
        fi
    fi

    # If we get here, we're finished with the tarball
    # If archiveFile hasn't moved it, then we'll remove it here
    if [ -r ${DATA_FILE} ] ; then
        rm -f ${DATA_FILE}
    fi

    # As we've either moved or deleted the DATA_FILE,
    # then set DELETE_DATA_FILE=0
    # The purpose of this is to prevent processFile from deleting a
    # file that is uploaded between here and when makeStats finishes
    DELETE_DATA_FILE=0

    #
    # Process extracted data
    #
    if [ "${FILE_TYPE}" = "Data" ] ; then
        if [ -d ../${DATE} ] ; then
            rm -rf ../${DATE}
        fi
        mv -f ${DATE} ../${DATE}
    elif [ "${FILE_TYPE}" = "Delta" ] ; then
        mv -f ${DATE}/* ../${DATE}
        rm -rf ${DATE}
    fi

    #make dirs for logging
    PROCESSED_LOG_DIRECTORY="${LOG_DIR}/`date +%F`"
    if [ ! -d $PROCESSED_LOG_DIRECTORY ] ; then
        mkdir -p $PROCESSED_LOG_DIRECTORY
    fi

    SLASH_DATE=`echo ${DATE} | sed 's/\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)/\1-\2-\3/g'`
    MAKESTATS_CMD="${ANALYSIS_BIN}/main/makeStats ${SLASH_DATE} ${SITE_NAME} ${DATA_DIR}/${SITE_TYPE} ${SITE_TYPE} ${FILE_TYPE} ${CONFIG_FILE} ${PRIVATE_IP} ${FQDN}"
    MAKESTATS_LOG="$PROCESSED_LOG_DIRECTORY/${SITE_NAME}-${DATE}.log"

    if [ -r ${MAKESTATS_LOG} ] ; then
        LOG_INDEX=1
        while [ -r ${MAKESTATS_LOG}.${LOG_INDEX}.gz ] ; do
            LOG_INDEX=$(expr ${LOG_INDEX} + 1)
        done
        mv ${MAKESTATS_LOG} ${MAKESTATS_LOG}.${LOG_INDEX}
        gzip ${MAKESTATS_LOG}.${LOG_INDEX}
    fi

    # If it's a delta archive, we need to update the ARCHIVE_INDEX
    if [ "${FILE_TYPE}" = "Delta" ] ; then
        local FILE_INDEX=$(echo ${FILE_ONLY} | awk -F_ '{print $4}' | sed 's/\.tar\.gz$//')
        echo "${FILE_INDEX}" > ${SITE_DIR}/data/${DATE}/ARCHIVE_INDEX
    fi

    if [ "${WORKER_HOSTNAME}" = "localhost" ] ; then
        ${MAKESTATS_CMD} > ${MAKESTATS_LOG} 2>&1
    elif [ "${WORKER_HOSTNAME}" = "kubernetes" ] ; then
        ERROR_LOG=$(${ANALYSIS_BIN}/main/runMakeStatsK8S ${INSTANCE} ${SITE_ID} "${MAKESTATS_CMD}" $MAKESTATS_LOG | egrep '^ERROR')
    else
        if [ ! -z "${WORKER_DB_HOST}" ] ; then
            MAKESTATS_CMD="export STATS_DB_HOST=${WORKER_DB_HOST} ; ${MAKESTATS_CMD}"
        fi
        ssh -q -n ${WORKER_HOSTNAME} "${MAKESTATS_CMD} > ${MAKESTATS_LOG} 2>&1"
    fi

    # Now cache the data by exec the index.php
    if [ "${FILE_TYPE}" = "Data" ] || [ "${FILE_TYPE}" = "Delta" ] ; then
        local SQL_DATE=`echo ${DATE} | sed 's/\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)/20\3-\2-\1/g'`
        local LC_SITE_TYPE=$(echo ${SITE_TYPE} | tr "[:upper:]" "[:lower:]")
        local URL=$(printf "https://localhost/php/index.php?dir=%s&date=%s&site=%s&oss=%s&refresh=1" ${DATE} ${SQL_DATE} ${SITE_NAME} ${LC_SITE_TYPE})
        /bin/wget --no-check-certificate --quiet -O /dev/null --user ddp --password ddp_passwd "${URL}"
    fi
}

processFile()
{
    LINE=$1

    local DATA_FILE=$(echo "${LINE}" | awk '{print $1}')
    local SITE_NAME=$(echo "${LINE}" | awk '{print $2}')
    local SITE_TYPE=$(echo "${LINE}" | awk '{print $3}')
    local SITE_ID=$(echo "${LINE}" | awk '{print $4}')
    local UPLOADED=$(echo "${LINE}" | awk '{print $5, $6}')

    local DATE=$(echo ${DATA_FILE} | awk -F\/ '{print $NF}' | awk -F_ '{print $3}' | sed 's/\.tar\.gz$//')
    local KEY="${DATE}-${SITE_NAME}"

    local BEGIN_PROC_TIME=$(date '+%Y-%m-%d %H:%M:%S')

    # Use flock to ensure that we don't try and process files for the same site/date concurrently
    # (applies to Delta files)
    SITE_LOCK_FILE=${SITE_LOCK_DIR}/${KEY}
    exec 200>${SITE_LOCK_FILE}
    flock --exclusive 200

    ERROR=""
    if [ -r ${INCOMING_ROOT}/${DATA_FILE} ] ; then
        # Normally, we'll delete the data file if extractAndProcess returns
        # without deleting. However there are certains cases where we don't
        # want this, in these cases extractAndProcess will set DELETE_DATA_FILE to 0
        DELETE_DATA_FILE=1

        #
        # Record info about how long it's taking to process
        # OSS_Data files
        #
        local FILE_INFO=`ls -l --time-style "+%Y-%m-%d %H:%M:%S" ${INCOMING_ROOT}/${DATA_FILE}`
        local FILE_SIZE=`echo ${FILE_INFO} | awk '{print $5}'`
        local FILE_SIZE_KB=`expr ${FILE_SIZE} / 1024`
        local SQL_DATE=`echo ${DATE} | sed 's/\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)\([0-9]\{2,2\}\)/20\3-\2-\1/g'`

        local SITE_STATUS=$(echo "SELECT site_status FROM sites WHERE id = ${SITE_ID}" | ${ANALYSIS_BIN}/sql/runSql)
        if [ "${SITE_STATUS}" = "inactive" ] ; then
            ERROR="Deleting ${DATA_FILE} as the site is inactive"
        else
            extractAndProcess ${INCOMING_ROOT}/${DATA_FILE} ${SITE_TYPE} ${SITE_NAME} ${DATE} ${FILE_SIZE} ${SITE_ID}
        fi

        # Normally extractAndProcess will have moved the DDC_Datat file to the archive but
        # in error cases we need to check/remove it here
        if [ -r ${INCOMING_ROOT}/${DATA_FILE} ] && [ ${DELETE_DATA_FILE} -eq 1 ] ; then
            rm -f ${INCOMING_ROOT}/${DATA_FILE}
        fi
    else
        FILE_SIZE_KB=0
        rm -f ${INCOMING_ROOT}/${DATA_FILE}
        ERROR="Deleting ${DATA_FILE} uploaded at ${UPLOADED} as it does not have correct read permissions."
    fi

    if [ -z "${ERROR}" ] ; then
        ERROR="NULL"
        local END_PROC_TIME=$(date "+'%Y-%m-%d %H:%M:%S'")
    else
        log "Failed to process ${SITE_NAME} ${DATE}: ${ERROR}"
        ERROR="'${ERROR}'"
        local END_PROC_TIME="NULL"
    fi

    local ARCHIVE_INDEX=0
    local ARCHIVE_INDEX_FILE=${DATA_DIR}/${SITE_TYPE}/${SITE_NAME}/data/${DATE}/ARCHIVE_INDEX
    if [ -s ${ARCHIVE_INDEX_FILE} ] ; then
        ARCHIVE_INDEX=$(cat ${ARCHIVE_INDEX_FILE})
    fi

    ${ANALYSIS_BIN}/sql/runSql <<EOF
use ${ADMINDB};
DELETE FROM file_processing WHERE file = '${DATA_FILE}';
INSERT INTO ddp_makestats
 (siteid,filesize,filedate,uploaded,beginproc,endproc,error)
 VALUES
 (
   ${SITE_ID}, ${FILE_SIZE_KB}, '${SQL_DATE}',
   '${UPLOADED}', '${BEGIN_PROC_TIME}', ${END_PROC_TIME}, ${ERROR}
 );

DELETE FROM file_processed WHERE siteid = '${SITE_ID}' AND file_date = '${SQL_DATE}' ;
INSERT INTO file_processed (siteid,file_date,archive_index) VALUES (${SITE_ID},'${SQL_DATE}',${ARCHIVE_INDEX});
EOF

    #
    # We get called here as a child process so make sure we exit
    #
    exit
}

doWork() {
    KEEP_GOING=1
    DEBUG=0

    cat <<EOF > ${WORKER_LOG_DIR}/workeractive.sql
SELECT active FROM ${ADMINDB}.workers WHERE id = ${WORKER_ID}
EOF

    while [ ${KEEP_GOING} -ne 0 ] ; do
        if [ -r /tmp/debug_worker.${WORKER_HOSTNAME} ] ; then
            DEBUG=1
            set -xv
        else
            DEBUG=0
            set +xv
        fi

        NUM_ACTIVE=$(jobs | wc -l)
        if [ ${DEBUG} -eq 1 ] ; then
            date
            pstree -a $$
            jobs
            echo "NUM_ACTIVE=${NUM_ACTIVE}"
        fi

        while [ ${NUM_ACTIVE} -ge ${MAX_JOBS} ] ; do
            sleep 5
            NUM_ACTIVE=$(jobs | grep -c Running)
            if [ ${DEBUG} -eq 1 ] ; then
                date
                if [ -x /usr/bin/ptree ] ; then
                    /usr/bin/ptree $$
                fi
                jobs
                echo "NUM_ACTIVE=${NUM_ACTIVE}"
            fi
        done

        if [ -r ${EXIT_FILE} ] ; then
            echo "doWork: Found exit file"
            break;
        fi

        #
        # Check that this worker is still active
        #
        WORKER_ACTIVE=$(${ANALYSIS_BIN}/sql/runSql ${WORKER_LOG_DIR}/workeractive.sql)
        if [ "${WORKER_ACTIVE}" != 1 ] ; then
            echo "doWork: Worker not active"
            break;
        fi

        # Each time around the loop, we need to make sure that we have a different value for timestamp,
        # otherwise the SELECT statement could pick up rows from the previous iteration,
        # so we sleep here for 2 secs
        sleep 2

        # Update ANALYSIS_BIN to based on whatever current is pointing at
        local CURRENT_ANALYSIS_BIN=$(realpath /data/ddp/current/analysis)
        if [ -z "${CURRENT_ANALYSIS_BIN}" ] ; then
            echo "doWork: Failed to get CURRENT_ANALYSIS_BIN"
            break
        elif [ "${CURRENT_ANALYSIS_BIN}" != "${ANALYSIS_BIN}" ] ; then
            echo "doWork: Updating ANALYSIS_BIN to ${CURRENT_ANALYSIS_BIN}"
            ANALYSIS_BIN=${CURRENT_ANALYSIS_BIN}
        fi

        SLOTS_AVAILABLE=$(expr ${MAX_JOBS} - ${NUM_ACTIVE})
        TIME_STAMP=$(date '+%Y-%m-%d %H:%M:%S')
        cat > ${WORKER_LOG_DIR}/getwork.sql <<EOF
use ${ADMINDB};
UPDATE file_processing
 SET starttime = '${TIME_STAMP}', workerid = ${WORKER_ID}
WHERE
  starttime IS NULL
ORDER BY priority DESC, date DESC, n_makestats ASC, deltaindex ASC, uploaded
LIMIT ${SLOTS_AVAILABLE};
SELECT file,site,sitetype,siteid,uploaded FROM file_processing WHERE workerid = '${WORKER_ID}' AND starttime='${TIME_STAMP}';
EOF
        ${ANALYSIS_BIN}/sql/runSql ${WORKER_LOG_DIR}/getwork.sql > ${WORKER_LOG_DIR}/work.txt
        if [ $? -ne 0 ] ; then
            log "doWork: Failed to query file_processing"
            break;
        fi

        NUM_FOUND=$(cat ${WORKER_LOG_DIR}/work.txt | wc -l | awk '{print $1}')
        if [ ${NUM_FOUND} -gt 0 ] ; then
            while read LINE ; do
                processFile "${LINE}" &
            done < ${WORKER_LOG_DIR}/work.txt
        else
            # Keep looping until there are no on-going jobs
            if [ ${NUM_ACTIVE} -eq 0 ] ; then
                KEEP_GOING=0
            fi
        fi
    done

    wait
}

ANALYSIS_BIN=$(dirname $0)
ANALYSIS_BIN=$(cd ${ANALYSIS_BIN} ; cd .. ; pwd)

if [ -z "$2" ] || [ ! -r "$1" ] ; then
    echo "Usage: $0 configfile workerhost"
    exit 1
fi

CONFIG_FILE=$1
. ${CONFIG_FILE}

WORKER_HOSTNAME=$2

EXIT_FILE=/tmp/ddp_worker.exit.${WORKER_HOSTNAME}

if [ -z "${MIN_SEC_SINCE_MODIFIED}" ] ; then
    MIN_SEC_SINCE_MODIFIED=60
fi

if [ ! -z "${MYSQL_HOST}" ] ; then
    export MYSQL_HOST
    export STATS_DB_HOST=${MYSQL_HOST}
fi

if [ -z "${LOG_DIR}" ] ; then
    LOG_DIR=/data/ddp/log
fi
if [ ! -d ${LOG_DIR} ] ; then
    mkdir ${LOG_DIR}
fi

WORKER_ID=$(echo "SELECT id FROM ddpadmin.workers WHERE ddpadmin.workers.hostname = '${WORKER_HOSTNAME}'" | ${ANALYSIS_BIN}/sql/runSql)
if [ -z "${WORKER_ID}" ] ; then
    log "ERROR: Could not get id for worker ${WORKER_HOSTNAME}"
    exit 1
fi

WORKER_MAX_JOBS=$(echo "SELECT ddpadmin.workers.max_jobs FROM ddpadmin.workers WHERE ddpadmin.workers.id = ${WORKER_ID} AND ddpadmin.workers.max_jobs IS NOT NULL" | ${ANALYSIS_BIN}/sql/runSql)
if [ ! -z "${WORKER_MAX_JOBS}" ] ; then
    echo "INFO: Setting MAX_JOBS to ${WORKER_MAX_JOBS}"
    MAX_JOBS=${WORKER_MAX_JOBS}
fi

if [ -z "${MAX_JOBS}" ] ; then
    echo "WARN: MAX_JOBS not set, assuming a value of 1"
    MAX_JOBS=1
fi


RUNNING_LOG_DIR=/data/ddp/log
[ -z "${FINAL_LOG_DIR}" ] && FINAL_LOG_DIR=/data/ddp/log

SITE_LOCK_DIR=/data/tmp/site_lock
if [ ! -d ${SITE_LOCK_DIR} ] ; then
    mkdir ${SITE_LOCK_DIR}
fi

WORKER_LOG_DIR=/data/tmp/ddp_worker.${WORKER_HOSTNAME}
if [ ! -d ${WORKER_LOG_DIR} ] ; then
    mkdir ${WORKER_LOG_DIR}
else
    find ${WORKER_LOG_DIR} -type f -exec rm -f {} \;
fi

# When we start, make sure that there are no "hanging entries" in the file_processing table
cat > ${WORKER_LOG_DIR}/clearfileprocessing.sql <<EOF
use ${ADMINDB};
SELECT * FROM file_processing WHERE workerid = ${WORKER_ID} AND starttime IS NOT NULL;
UPDATE file_processing SET starttime = NULL WHERE workerid = ${WORKER_ID}';
EOF
${ANALYSIS_BIN}/sql/runSql ${WORKER_LOG_DIR}/clearfileprocessing.sql > ${WORKER_LOG_DIR}/clearfileprocessing.start 2>&1

PRIVATE_IP=$(getent hosts ${INSTANCE}-priv | awk '{print $1}')
if [ -z "${PRIVATE_IP}" ] ; then
    PRIVATE_IP=127.0.0.1
fi

DO_WORK_EVERY=6
WORK_COUNT=${DO_WORK_EVERY}
while [ ! -r ${EXIT_FILE} ] ; do
    if [ ${WORK_COUNT} -eq ${DO_WORK_EVERY} ] ; then
        doWork
        WORK_COUNT=0
    else
        WORK_COUNT=$(expr ${WORK_COUNT} + 1)
    fi

    if [ ! -r ${EXIT_FILE} ] ; then
        sleep 10
    fi
done

rm -f ${EXIT_FILE}

${ANALYSIS_BIN}/sql/runSql ${WORKER_LOG_DIR}/clearfileprocessing.sql > ${WORKER_LOG_DIR}/clearfileprocessing.end 2>&1
